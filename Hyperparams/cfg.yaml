defaults:
  - _self_
  - task@_global_: atari/pong
  - override hydra/launcher: submitit_local

# Agent
obs_shape: ???  # To be specified later
action_shape: ??? # To be specified later
trunk_dim: 50
hidden_dim: 1024
explore_steps: 2000
lr: 1e-4
# Domains
discrete: ??? # To be specified later
offline: false
RL: true
supervise: true
generate: false
# Replay
obs_spec: ??? # To be specified later
action_spec: ??? # To be specified later
nstep: 10
batch_size: 256
discount: 0.99
# Training
seed_steps: 2000
learn_per_steps: 2
learn_steps_after: 1000
# Evaluating
evaluate_per_steps: 5000
evaluate_episodes: 10
ema: false
# Saving
save: true
save_per_steps: 0
save_path: ./Checkpoints/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Agent.pt
load: false
load_per_steps: 0
# Logging
log_video: ${generate}
log_per_episodes: 1
# Plotting
plot_per_steps: 10000
# Misc
device: ??? # To be specified later
parallel: false
num_workers: 4
# Experiment
Agent: Agents.DQNAgent
seed: 1
experiment: Exp

environment:
  _target_: Datasets.Environment.Environment
  task_name: ${task_name}
  frame_stack: ${frame_stack}
  action_repeat: ${action_repeat}
  max_episode_frames: null
  truncate_episode_frames: 1000
  seed: ${seed}
  suite: ${suite}
  offline: ${offline}
  generate: ${generate}
  batch_size: 1000
  num_workers: ${num_workers}

agent:
  _target_: ${Agent}
  _recursive_: false
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  trunk_dim: ${trunk_dim}
  hidden_dim: ${hidden_dim}
  recipes: ${recipes}
  lr: ${lr}
  ema_tau: 0.01
  ema: ${ema}
  explore_steps: ${explore_steps}
  stddev_schedule: ${stddev_schedule} # Specified in task/100K,500K,etc.
  stddev_clip: 0.3
  discrete: ${discrete}
  RL: ${RL}
  supervise: ${supervise}
  generate: ${generate}
  device: ${device}
  parallel: ${parallel}
  log: false

replay:
  _target_: Datasets.ExperienceReplay.ExperienceReplay
  obs_spec: ${obs_spec}
  action_spec: ${action_spec}
  capacity: 1000000
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  nstep: ${nstep}
  discount: ${discount}
  offline: ${offline}
  generate: ${generate}
  save: false
  load: false
  path: ./Datasets/ReplayBuffer/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Memories

logger:
  _target_: Logger.Logger
  path: ./Benchmarking/${experiment}/${Agent}/${environment.suite}/
  task: ${task_name}
  seed: ${seed}

vlogger:
  _target_: Vlogger.Vlogger
  path: ./Benchmarking/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Video_Image
  fps: 20
  reel: ${generate}

plotting:
  _target_: Plot.plot
  path: ./Benchmarking/${experiment}/Plots
  plot_experiments: ${experiment}
  plot_agents: null
  plot_suites: null
  plot_tasks: null
  steps: ${train_steps}
  plot_tabular: false

# Can optionally pass in custom architectures such as those defined in ./Blocks/Architectures
recipes:
  Encoder:
    Eyes: null
    pool: null
  Actor:
    trunk: null
    Pi_head: null
  Critic:
    trunk: null
    Q_head: null
  Aug: null
  encoder:
    eyes:
      _target_: ${recipes.Encoder.Eyes}
      input_shape: ${obs_shape}
      output_dim: null
    pool:
      _target_: ${recipes.Encoder.pool}
      input_shape: null
      output_dim: 1024
  actor:
    trunk:
      _target_: ${recipes.Actor.trunk}
      input_shape: null
      output_dim: ${trunk_dim}
    pi_head:
      _target_: ${recipes.Actor.Pi_head}
      input_shape:
        - ${trunk_dim}
  critic:
    trunk:
      _target_: ${recipes.Critic.trunk}
      input_shape: null
      output_dim: ${trunk_dim}
    q_head:
      _target_: ${recipes.Critic.Q_head}
      input_shape: null
  aug:
    _target_: ${recipes.Aug}

hydra:
  run:
    dir: ./
  sweep:
    dir: ./
    subdir: ./
  launcher:
    timeout_min: 4300
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: ./
