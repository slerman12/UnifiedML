defaults:
  - _self_
  - task@_global_: atari/pong
  - override hydra/job_logging: none

# Agent
obs_shape: ???  # To be specified later
action_shape: ??? # To be specified later
data_norm: null
trunk_dim: 50
hidden_dim: 1024
explore_steps: 2000
lr: 1e-4
lr_decay_epochs: 0
weight_decay: 0
ema_decay: 0.99
# Domains
discrete: ??? # To be specified later
offline: false  # Defaults to true for classify tasks
RL: true
supervise: true
generate: false
# Replay
obs_spec: ??? # To be specified later
action_spec: ??? # To be specified later
nstep: 10
batch_size: 256
discount: 0.99
transform: null
# Training
seed_steps: 2000
learn_per_steps: 2
learn_steps_after: 10000
# Evaluating
evaluate_per_steps: 5000
evaluate_episodes: 10
ema: false
# Saving
save: true
save_per_steps: 0
save_path: ./Checkpoints/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Agent.pt
load: false
load_per_steps: 0
# Logging
log_video: ${generate}
log_per_episodes: 1
# Plotting
plot_per_steps: 50000
# Misc
device: null
parallel: false
num_workers: 4
# Experiment
Agent: Agents.DQNAgent
seed: 1
experiment: Exp

environment:
  _target_: Datasets.Environment.Environment
  task_name: ${task_name}
  frame_stack: ${frame_stack}
  action_repeat: ${action_repeat}
  episode_max_frames: false
  episode_truncate_resume_frames: 1000
  seed: ${seed}
  suite: ${suite}
  offline: ${offline}
  generate: ${generate}
  batch_size: ${batch_size}
  num_workers: ${num_workers}

agent:
  _target_: ${Agent}
  _recursive_: false
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  trunk_dim: ${trunk_dim}
  hidden_dim: ${hidden_dim}
  data_norm: ${data_norm}
  recipes: ${recipes}
  lr: ${lr}
  lr_decay_epochs: ${lr_decay_epochs}
  weight_decay: ${weight_decay}
  ema_decay: ${ema_decay}
  ema: ${ema}
  explore_steps: ${explore_steps}
  stddev_schedule: ${stddev_schedule} # Specified per task
  stddev_clip: 0.3
  discrete: ${discrete}
  RL: ${RL}
  supervise: ${supervise}
  generate: ${generate}
  device: ${device}
  parallel: ${parallel}
  log: false

replay:
  _target_: Datasets.ExperienceReplay.ExperienceReplay
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  capacity: 1000000
  obs_spec: ${obs_spec}
  action_spec: ${action_spec}
  suite: ${suite}
  task: ${task_name}
  nstep: ${nstep}
  discount: ${discount}
  transform: ${transform}
  offline: ${offline}
  generate: ${generate}
  save: false
  load: false
  path: ./Datasets/ReplayBuffer/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Memories

logger:
  _target_: Logger.Logger
  path: ./Benchmarking/${experiment}/${Agent}/${environment.suite}/
  task: ${task_name}
  seed: ${seed}
  wandb: False

vlogger:
  _target_: Vlogger.Vlogger
  path: ./Benchmarking/${experiment}/${Agent}/${environment.suite}/${task_name}_${seed}_Video_Image
  fps: 20
  reel: ${generate}

plotting:
  _target_: Plot.plot
  path: ./Benchmarking/${experiment}/Plots
  plot_experiments: ${experiment}
  plot_agents: null
  plot_suites: null
  plot_tasks: null
  steps: ${train_steps}
  write_tabular: false
  plot_bar: true
  verbose: false

# Can optionally pass in custom architectures such as those defined in ./Blocks/Architectures
Eyes: null  # Shorthand for recipes.encoder.eyes._target_
pool: null  # Shorthand for recipes.encoder.pool._target_
Pi_trunk: null  # Shorthand for recipes.actor.trunk._target_
Q_trunk: null  # Shorthand for recipes.critic.trunk._target_
Generator: null  # Shorthand for Pi_head --> recipes.actor.Pi_head._target_
Discriminator: null  # Shorthand for Q_head --> recipes.critic.Q_head._target_
Pi_head: ${Generator}  # Shorthand for recipes.actor.Pi_head._target_
Q_head: ${Discriminator}  # Shorthand for recipes.critic.Q_head._target_

recipes:
  Aug: null  # Shorthand for recipes.aug._target_
  aug:
    _target_: ${recipes.Aug}
  encoder:
    eyes:
      _target_: ${Eyes}
      input_shape: ${obs_shape}
      output_dim: null
    pool:
      _target_: ${pool}
      input_shape: null
      output_dim: null
  actor:
    trunk:
      _target_: ${Pi_trunk}
      input_shape: null
      output_dim: ${trunk_dim}
    pi_head:
      _target_: ${Pi_head}
      input_shape:
        - ${trunk_dim}
  critic:
    trunk:
      _target_: ${Q_trunk}
      input_shape: null
      output_dim: ${trunk_dim}
    q_head:
      _target_: ${Q_head}
      input_shape: null

hydra:
  run:
    dir: ./
  sweep:
    dir: ./
    subdir: ./
